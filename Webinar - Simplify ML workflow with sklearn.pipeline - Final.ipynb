{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to Simplify Machine Learning Workflow with scikit-learn Pipeline\n",
    "\n",
    "**Present to you by:** Siraprapa Watakit, Data Scientist, DACoE Asia\n",
    "<br>**Duration** : 1 Hour\n",
    "\n",
    "**Objective:** <font color=turguise> **Zero** to **Hero** Machine Learning with scikit-learn Pipeline.</font>\n",
    "\n",
    "**Prerequisite:**  \n",
    "- Basic python programming. \n",
    "- Basic understanding scikit-learn Estimator,Transformer API.\n",
    "- Basic knowledge in machine learning workflow\n",
    "\n",
    "<font color=darkred>  **Bonus:** lots of examples, and give aways functions and transformers. </font>\n",
    "\n",
    "<i>This notebook is powered by <b>RISE</b>, best viewed in Slideshow mode</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Agenda**\n",
    " - Background and Reviews : machine learning workflows, sklearn  Estimator,Transformer API.\n",
    " - **Basic** : Build a model with and without sklearn.Pipeline\n",
    " - **Advanced** : Build your own Customer Transformer and Model Wrapper\n",
    "\n",
    "**References**\n",
    " - Python Data Science Handbook\n",
    " - Kaggle competitor : Zac Stewart\n",
    " - PyData Conferences : Julie Michelman, Kevin Goetsch\n",
    " - PyCon Conferences : Kevin Markham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background and Reviews\n",
    "\n",
    "<img src=\"./images/ml_workflow.png\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data\n",
    "\n",
    " - <code>numpy </code> : a 1D or 2D numeric arrays, accessible via slicing notation eg. <code>X[<i>start:stop:step</i>]</code> where start:stop:step are positional integer\n",
    " - <code>pandas</code> : a Dataframe with row & column labeled, can be accessed via slicing notation as well as column names <br> e.g. <code> house_data.loc[:10,['GrLivArea','YearBuilt','SalePrice']]</code>\n",
    "\n",
    "\n",
    "# Preprocessing with Scikit-Learn’s Transformer API\n",
    "1. instantiate a <code>transformer</code> object\n",
    "2. <code>.fit</code> the <code>transformer</code> with data\n",
    "3. <code>.transform</code> on data\n",
    "\n",
    "# Modeling with Scikit-Learn’s Estimator API\n",
    "1. instantiate a <code>estimator</code> object\n",
    "2. <code>.fit</code> the <code>estimator</code> with data\n",
    "3. Supervised vs. Unsupervised\n",
    "    - Supervised <code>.predict</code> on data\n",
    "    - Unsupervised <code>.transform</code> on data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What happen when we invoke <code>.fit, .transform, .predict</code>\n",
    "<br>\n",
    "<img src=\"./images/fit_transform_predict.png\" >\n",
    "\n",
    "\n",
    "### <font color=darkred>  This is important, you need to understand it,<br> if you are to create your own Custom Transformer..</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example#1: Simple preprocessing and modeling\n",
    "\n",
    "Data : House Price(<a href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\">Kaggle</a>) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "house_data = pd.read_csv('./data/house_data.csv')\n",
    "house_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = house_data['SalePrice']\n",
    "X = house_data.loc[:,['GrLivArea','LotArea','Street']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #1: (1/4) Pre-process numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feat_num = ['GrLivArea','LotArea']\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X_train[feat_num])\n",
    "X_train_num_scaled = std_scaler.transform(X_train[feat_num])\n",
    "print(\"Scaled features:\\n\",X_train_num_scaled[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #1: (2/4)  Pre-process categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "feat_cat = ['Street']\n",
    "\n",
    "le = LabelEncoder()\n",
    "X_train[feat_cat] = le.fit_transform(X_train[feat_cat])\n",
    "ohe = OneHotEncoder(categorical_features = [0])\n",
    "X_train_cat_ohe = ohe.fit_transform(X_train[feat_cat]).toarray()\n",
    "\n",
    "print(\"Dummified features:\\n\",X_train_cat_ohe[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### This is only one of many ways to to stuffs with python!\n",
    "\n",
    "Check this out : <a href=\"https://www.youtube.com/watch?v=0s_1IsROgDc\">How do I create dummy variables in pandas?</a>\n",
    "\n",
    "<font color=darkred> NOTE: sklearn library takes care of dummy variable trap; hence even if you don't drop one of the columns, it is still going to work. However we should make a habit of taking care of dummy variable trap by ourselves. Just in case you are not using sklearn for modeling</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example#1: (3/4) Build a model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train_processed = np.concatenate((X_train_num_scaled, X_train_cat_ohe), axis=1)\n",
    "\n",
    "print(\"Processed Train Data: \\n\",X_train_processed[0:3],\"\\n\")\n",
    "lr1 = LinearRegression()\n",
    "lr1.fit(X_train_num_scaled,y_train)\n",
    "print(\"Model #1 :  Only 2 numeric features\")\n",
    "print(\"Intercept:\", lr1.intercept_)\n",
    "print(\"Coefficient:\", lr1.coef_)\n",
    "\n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(X_train_processed,y_train)\n",
    "print(\"\\nModel #2 : 2 numeric and 1 categorical features\")\n",
    "print(\"Intercept:\", lr2.intercept_)\n",
    "print(\"Coefficient:\", lr2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #1: (4/4) Pre-process the test data, then test and compare the model(s)\n",
    "\n",
    "Be careful with this part..\n",
    "- Use the estimator/transformer that were fitted with the <font color=darkred>**TRAIN DATA**</font>\n",
    "    - Estomators: <code>lr1,lr2</code>\n",
    "    - Transformer: <code>std_scaler,le,ohe</code>\n",
    "- Apply estimator/transformer with the <font color=blue>**TEST DATA** </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_test_num_scaled = std_scaler.transform(X_test[feat_num])\n",
    "X_test[feat_cat] = le.transform(X_test[feat_cat])         \n",
    "X_test_cat_ohe = ohe.transform(X_test[feat_cat]).toarray()\n",
    "X_test_processed = np.concatenate((X_test_num_scaled, X_test_cat_ohe), axis=1)\n",
    "\n",
    "print(\"Processed Test Data: \\n\",X_test_processed[0:3],\"\\n\")\n",
    "\n",
    "y_pred = lr1.predict(X_test_num_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Model #1 :  Only 2 numeric features\")\n",
    "print('The RMSE value is {:.4f}'.format(rmse))\n",
    "\n",
    "y_pred = lr2.predict(X_test_processed)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\nnModel #2 : 2 numeric and 1 categorical features\")\n",
    "print('The RMSE value is {:.4f}'.format(rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Do more with Pipeline\n",
    "\n",
    "### What is Pipeline\n",
    "- Pipeline is a container of steps / sequences of actions : <b><font color=darkred>list of tuples</font></b>\n",
    "- What can be in the <b><font color=darkred>list of tuples</font></b>\n",
    "    - <code>Transformer</code>\n",
    "    - <code>Estimator</code>\n",
    "    - <code>FeatureUnion</code>\n",
    "    - more <code>Pipeline</code> !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building Blocks of scikit-learn Pipeline \n",
    "<br>\n",
    "<img src=\"./images/block_of_legos.png\" >\n",
    "\n",
    "- Imagine each Transformer and Estimator is a piece of Lego, <code>Pipeline</code> make it possible to chain one block to another, <br><i><b>sending output of <code>block#1</code> as an input of<code>block#2</code></b></i>\n",
    "- <code>FeatureUnion</code> make it possible to aggregate output from multiple <code>Pipeline</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### <font color=darkred> Important Note </font>\n",
    "- Each block must have  <code>.fit, .transform </code> implemented\n",
    "- To create your own Customer Transformer, you need to inherit <code>BaseEstimator,TransformerMixin </code>\n",
    "    - <code>BaseEstimator</code> gives your transformer the <code>.fit_transform</code> method, for free.\n",
    "    - <code>TransformerMixin</code> gives your transformer grid-searchable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class MyTransformer(TransformerMixin, BaseEstimator):\n",
    "    '''A template for a custom transformer.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        # this is where you can init internal variable\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # this is what happend when .fit is invoked\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # this is what happend when .tranform is invoked\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #2: Pipeline with <code> Transformer - StandandSacler</code>\n",
    "<img src=\"./images/ex2.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipe = Pipeline([('std_scaler',StandardScaler())])\n",
    "num_pipe.fit(X_train[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color=darkred> One line transform !! </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_test_num_scaled_new = num_pipe.transform(X_test[feat_num])\n",
    "\n",
    "#This is the exact number shown in Example #1\n",
    "print(\"Processed Test Data - numeric feature:\\n\",X_test_num_scaled_new[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #3: Pipeline with <code> Transformer - Imputer and MinMaxScaler</code>\n",
    "<img src=\"./images/ex3.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_pipe = Pipeline([('impute',SimpleImputer(strategy='median')),\n",
    "                     ('minmax_scaler',MinMaxScaler())])\n",
    "num_pipe.fit(X_train[feat_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color=darkred> One line transform !! </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_test_num_scaled_new = num_pipe.transform(X_test[feat_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Processed Test Data - numeric feature:\\n\",X_test_num_scaled_new[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #4: Pipeline with <code> Transformer - StandardScaler , Estimator - LinearRegression</code>\n",
    "<img src=\"./images/ex4.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_pipe_model = Pipeline([('std_scaler',StandardScaler()),\n",
    "                           ('lr',LinearRegression())])\n",
    "num_pipe_model.fit(X_train[feat_num],y_train)\n",
    "\n",
    "print(\"Model #1 :  Only 2 numeric features\")\n",
    "print(\"Intercept:\",num_pipe_model.named_steps['lr'].intercept_)\n",
    "print(\"Coefficient:\", num_pipe_model.named_steps['lr'].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color=darkred> One line predict !! </font>\n",
    "- When <code>.predict</code> of a Pipeline is invoked, <code>Pipeline</code> will invoke a chain of <code>.transform</code>, from the top; followed by <code>.predict</code> of the estimator  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = num_pipe_model.predict(X_test[feat_num])\n",
    "\n",
    "#This is the exact number shown in Example #1 - Model #1\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('The RMSE value is {:.4f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #5: (1/2)Pipeline with <code>FeatureUnion and CustomTransformer</code>\n",
    "- Let's make it more dynamic, let's suppose we would like to **extract any features for any transformers**\n",
    "- First, we define <code>MyFeatureExtractor</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Let's dictate what's going to happend when an object is instanciated, fit, transform\n",
    "class MyFeatureExtractor(TransformerMixin, BaseEstimator):\n",
    "    '''This customer tranformer extracts and returns specific feature.'''\n",
    "\n",
    "    def __init__(self,feature):\n",
    "        # init feature to extract\n",
    "        self.feature = feature\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # do nothing\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # return a sub-seleted feature\n",
    "        return X[self.feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #5: (2/2) Fomulate feature engineering strategy, and construct FeatureUnion Pipeline\n",
    "\n",
    "<img src=\"./images/ex5.png\" >\n",
    "\n",
    "- Let's suppose we would like to extract and..\n",
    "    - Standize <code>GrLivArea,LotArea,YearBuilt</code>\n",
    "    - MinMaxStandize <code>OverallQual,OverallCond</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "COLS_1 = ['GrLivArea','LotArea','YearBuilt']\n",
    "COLS_2 = ['OverallQual','OverallCond']\n",
    "\n",
    "my_pipelines = FeatureUnion([('std',Pipeline([('cols1',MyFeatureExtractor(COLS_1)),\n",
    "                                              ('std',StandardScaler() )])  ),\n",
    "                             ('minmax',Pipeline([('cols2',MyFeatureExtractor(COLS_2)),\n",
    "                                                 ('minmax',MinMaxScaler() )]))])\n",
    "\n",
    "my_pipelines.fit(house_data)\n",
    "output = my_pipelines.transform(house_data)\n",
    "print(output[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #6: Pipeline with <code>FeatureUnion + CustomTransformer + Estimator(Unsupervised)</code>\n",
    "\n",
    "<img src=\"./images/ex6.png\" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "my_pipelines = Pipeline([('all_features',FeatureUnion([('std',Pipeline([('cols1',MyFeatureExtractor(COLS_1)),\n",
    "                                                                        ('std',StandardScaler() )])  ),\n",
    "                                                       ('minmax',Pipeline([('cols2',MyFeatureExtractor(COLS_2)),\n",
    "                                                                           ('minmax',MinMaxScaler() )]))])),\n",
    "                        ('pca',PCA(n_components=1))])\n",
    "my_pipelines.fit(house_data)\n",
    "output = my_pipelines.transform(house_data)\n",
    "print(output[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #7: Pipeline with <code>FeatureUnion + CustomTransformer + Estimator(Supervised)</code>\n",
    "\n",
    "<img src=\"./images/ex7.png\" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_pipelines = Pipeline([('all_features',FeatureUnion([('std',Pipeline([('cols1',MyFeatureExtractor(COLS_1)),\n",
    "                                                                        ('std',StandardScaler() )])  ),\n",
    "                                                       ('minmax',Pipeline([('cols2',MyFeatureExtractor(COLS_2)),\n",
    "                                                                           ('minmax',MinMaxScaler() )]))])),\n",
    "                        ('lr',LinearRegression())])\n",
    "\n",
    "my_pipelines.fit(house_data,house_data['SalePrice'])\n",
    "\n",
    "print(\"Intercept:\",my_pipelines.named_steps['lr'].intercept_)\n",
    "print(\"Coefficient:\", my_pipelines.named_steps['lr'].coef_)\n",
    "\n",
    "output = my_pipelines.predict(house_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "price_compare = pd.concat([house_data['SalePrice'],pd.DataFrame(output)],axis=1)\n",
    "price_compare.columns = ['SalePrice','Predicted']\n",
    "\n",
    "print(price_compare.head())\n",
    "rmse = np.sqrt(mean_squared_error(price_compare['SalePrice'], price_compare['Predicted']))\n",
    "print('\\nThe RMSE value is {:.4f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #8: Pipeline with <code>FeatureUnion + CustomTransformer + Estimator(Supervised) + GridSearch</code>\n",
    "- Let's do GridSearch with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "my_pipelines = Pipeline([('all_features',FeatureUnion([('std',Pipeline([('cols1',MyFeatureExtractor(COLS_1)),\n",
    "                                                                        ('std',StandardScaler() )])  ),\n",
    "                                                       ('minmax',Pipeline([('cols2',MyFeatureExtractor(COLS_2)),\n",
    "                                                                           ('minmax',MinMaxScaler() )]))])),\n",
    "                        ('ridge',Ridge())])\n",
    "\n",
    "params = {'ridge__alpha': [1,0.1,0.01,0.001,0.0001,0]}\n",
    "grid = GridSearchCV(estimator=my_pipelines, param_grid=params)\n",
    "\n",
    "grid.fit(house_data,house_data['SalePrice'].values)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #9: Finally, publish your model !!\n",
    "- Note: Using whole dataset(house_data) for convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Build the final model\n",
    "\n",
    "my_pipelines = Pipeline([('all_features',FeatureUnion([('std',Pipeline([('cols1',MyFeatureExtractor(COLS_1)),\n",
    "                                                                        ('std',StandardScaler() )])  ),\n",
    "                                                       ('minmax',Pipeline([('cols2',MyFeatureExtractor(COLS_2)),\n",
    "                                                                           ('minmax',MinMaxScaler() )]))])),\n",
    "                        ('ridge',Ridge(alpha=0.01))])\n",
    "my_pipelines.fit(house_data,house_data['SalePrice'])\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "#Save file to disk\n",
    "joblib.dump(my_pipelines, './output/final_pipelines.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's test it - with a completely new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "house_test = pd.read_csv('./data/house_test.csv')\n",
    "house_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loaded_pipelines = joblib.load('./output/final_pipelines.sav')\n",
    "predicted_prices = loaded_pipelines.predict(house_test)\n",
    "pd.DataFrame(predicted_prices).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Check-in\n",
    "\n",
    "- So far, we see the the ouput of Pipeline are in <code>numpy</code> format, what if we would like to stay in <code>pandas</code> land?\n",
    "- There isn't an example of <code>Pipeline[('le',LabelEncoder),('ohe',OneHotEncoder())]</code> here? <a href=\"https://github.com/scikit-learn/scikit-learn/issues/3956\">issues#3956</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Solution to this - Custom Transformer, but let's not reinvent the wheels\n",
    "\n",
    "**Julie Michelman** - <a href=\"https://www.youtube.com/watch?v=BFaadIqWlAg\">PyCon 2016</a>\n",
    "\n",
    "Available Customer Transformers\n",
    "- DFImputer\n",
    "- DFStandardScaler\n",
    "- ZeroFillTransformer\n",
    "- Log1pTransformer\n",
    "- DateFormatter\n",
    "- DummyTransformer\n",
    "- MultiEncoder\n",
    "\n",
    "<b><i><font color=darkred>and lots more..<font></i></b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #10:  Custom Transformers - DataFrame\n",
    "- First, let's take a look at mockup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mockup = pd.read_csv('./data/mockup_data.csv')\n",
    "print(mockup.head())\n",
    "\n",
    "#Let's check missing values\n",
    "mockup.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From what I saw above, here is what I would like to do\n",
    "- For categorical variables <code>Sex, Color</code>, create dummies\n",
    "- For numerric variables <code>SABal, CCBal,INVBal</code>, fill missing values with zeros and then take log1p\n",
    "- <font color=darkred><b>For business rules variable, do RIDITS transformation - Bross(1958)</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class RiditsTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"RIDITS tramsformation - Bross(1958)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        #initialie dictionaries\n",
    "        self.dict_ridits_1 = {}\n",
    "        self.dict_ridits_0 = {}\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #for each feature,build rules\n",
    "        Xr =pd.DataFrame(X)\n",
    "        rules = Xr.columns\n",
    "        for rule in rules:\n",
    "            try:\n",
    "                r = Xr[rule].value_counts()\n",
    "                p_1 = r[1]/np.sum(r)\n",
    "                p_0 = r[0]/np.sum(r)\n",
    "                ridit_1 = (0.5 * p_1)\n",
    "                ridit_0 = (p_1+0.5*p_0)\n",
    "                self.dict_ridits_1.update({str(rule):ridit_1})\n",
    "                self.dict_ridits_0.update({str(rule):ridit_0})\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # transform X with fitted dictionary\n",
    "        Xr =pd.DataFrame(X)\n",
    "        rules = Xr.columns\n",
    "        for rule in rules:\n",
    "            try:\n",
    "                #replace 0,1 with ridits\n",
    "                if str(rule) in self.dict_ridits_1.keys():\n",
    "                    ridit_1 = self.dict_ridits_1.get(str(rule))\n",
    "                    ridit_0 = self.dict_ridits_0.get(str(rule))\n",
    "                    Xr[rule]=Xr[rule].map({1:ridit_1,0:ridit_0})\n",
    "            except :\n",
    "                pass\n",
    "\n",
    "        return Xr\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Define a special transformer - RiditsTransformer\n",
    "- Please visit <a href=\"https://analytics.knowledgehub.ageas.com/posts/2005000-sas-python-by-example-anomaly-detection-with-pridit\">Knowledge Hub</a> for detailed explanation \n",
    "\n",
    "\n",
    "Or have a quick look at <a href=\"https://nbviewer.jupyter.org/github/swatakit/Python-Tools/blob/master/Anomaly%20Detection%20with%20PRIDIT%20-%20Python.ipynb\">RIDITS- Python</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from custom_transformers import (ColumnExtractor, DFStandardScaler, DFFeatureUnion, \n",
    "                                 DFImputer,DummyTransformer,ZeroFillTransformer,Log1pTransformer)\n",
    "\n",
    "feat_rules= ['R1','R2','R3','R4']\n",
    "feat_cats = ['Sex','Color']\n",
    "feat_nums = ['SABal','CCBal','INVBal']\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', DFFeatureUnion([\n",
    "        ('categoricals', Pipeline([\n",
    "            ('extract', ColumnExtractor(feat_cats)),\n",
    "            ('dummy', DummyTransformer())\n",
    "        ])),\n",
    "        ('numerics', Pipeline([\n",
    "            ('extract', ColumnExtractor(feat_nums)),\n",
    "            ('zero_fill', ZeroFillTransformer()),\n",
    "            ('log', Log1pTransformer())\n",
    "        ])),\n",
    "        ('ridits', Pipeline([\n",
    "            ('extract', ColumnExtractor(feat_rules)),\n",
    "            ('ridits', RiditsTransformer())\n",
    "        ]))\n",
    "    ]))\n",
    "])\n",
    "\n",
    "pipeline.fit(mockup)\n",
    "mockup_trans = pipeline.transform(mockup)\n",
    "mockup_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now, let suppose we have acquired new data\n",
    "- <b><font color=#FF00FF>Take notice on the last observation</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mockup_new = pd.read_csv('./data/mockup_new.csv')\n",
    "mockup_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mockup_new_trans = pipeline.transform(mockup_new)\n",
    "mockup_new_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #11:  Custom Transformers - DataFrame + Logistic Regression(<code>.predict</code>)\n",
    "- Let's add Logistic regression into it\n",
    "- Again, using the whole sample(mockup) just for convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline_logreg = Pipeline([\n",
    "    ('features', DFFeatureUnion([\n",
    "        ('categoricals', Pipeline([\n",
    "            ('extract', ColumnExtractor(feat_cats)),\n",
    "            ('dummy', DummyTransformer())\n",
    "        ])),\n",
    "        ('numerics', Pipeline([\n",
    "            ('extract', ColumnExtractor(feat_nums)),\n",
    "            ('zero_fill', ZeroFillTransformer()),\n",
    "            ('log', Log1pTransformer())\n",
    "        ])),\n",
    "        ('ridits', Pipeline([\n",
    "            ('extract', ColumnExtractor(feat_rules)),\n",
    "            ('ridits', RiditsTransformer())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('logreg',LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "pipeline_logreg.fit(mockup,mockup['Target'])\n",
    "y_pred = pipeline_logreg.predict(mockup)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example #12:  Custom Transformers - DataFrame + Logistic Regression(<code>.predict_proba</code>)\n",
    "- Suppose, we would like to have probability, instead of 0,1\n",
    "- We need to improvise by having a model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class ModelTransformer(TransformerMixin, BaseEstimator):\n",
    "    '''model.predict_proba'''\n",
    "\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self,*args, **kwargs):\n",
    "        #expecting X, and y\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self,X,**transform_params):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_logreg_proba = Pipeline([\n",
    "    ('features', DFFeatureUnion([\n",
    "        ('categoricals', Pipeline([\n",
    "            ('extract', ColumnExtractor(feat_cats)),\n",
    "            ('dummy', DummyTransformer())\n",
    "        ])),\n",
    "        ('numerics', Pipeline([\n",
    "            ('extract', ColumnExtractor(feat_nums)),\n",
    "            ('zero_fill', ZeroFillTransformer()),\n",
    "            ('log', Log1pTransformer())\n",
    "        ])),\n",
    "        ('ridits', Pipeline([\n",
    "            ('extract', ColumnExtractor(feat_rules)),\n",
    "            ('ridits', RiditsTransformer())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('logreg',ModelTransformer(LogisticRegression(solver='liblinear')))\n",
    "])\n",
    "\n",
    "pipeline_logreg_proba.fit(mockup,mockup['Target'])\n",
    "y_pred_proba = pipeline_logreg_proba.transform(mockup)\n",
    "y_pred_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color=darkred>Bonus: Complete 2-Class classification reports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from custom_functions import generate_miss_report\n",
    "\n",
    "titanic = pd.read_csv('./data/titanic_train.csv')\n",
    "titanic_missing = generate_miss_report(titanic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "feat_cats = ['Sex','Embarked']\n",
    "feat_nums = ['Age']\n",
    "feat_donth = ['Pclass','SibSp']\n",
    "\n",
    "pipeline_titanic = Pipeline([\n",
    "    ('features', DFFeatureUnion([\n",
    "        ('numerics', Pipeline([\n",
    "            ('extract', ColumnExtractor(feat_nums)),\n",
    "            ('mid_fill', DFImputer(strategy='median')),\n",
    "            ('scale', DFStandardScaler())\n",
    "        ])),\n",
    "        ('categoricals', Pipeline([\n",
    "            ('extract', ColumnExtractor(feat_cats)),\n",
    "            ('dummy', DummyTransformer())\n",
    "        ])),\n",
    "        ('raw', Pipeline([\n",
    "            ('extract', ColumnExtractor(feat_donth)),\n",
    "        ]))\n",
    "    ]))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "y = titanic['Survived']\n",
    "X = titanic.drop(columns=['Survived'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234, stratify=y)\n",
    "\n",
    "pipeline_titanic.fit(X_train)\n",
    "X_train_trans = pipeline_titanic.transform(X_train)\n",
    "\n",
    "X_train_trans.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "model.fit(X_train_trans,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from custom_functions import print_classification_performance2class_report\n",
    "\n",
    "acc,pc,rc,fs,ap,roc_auc,gini,rmse = print_classification_performance2class_report(model,pipeline_titanic.transform(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from custom_functions import print_gainlift_charts\n",
    "X_test_trans = pipeline_titanic.transform(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test_trans)[:,1]\n",
    "df_tmp = pd.DataFrame([y_pred_proba,y_test]).transpose()\n",
    "df_tmp.columns = ['logreg_proba','Survived']\n",
    "\n",
    "outtab = print_gainlift_charts(data=df_tmp,\n",
    "                               var_to_rank='logreg_proba',\n",
    "                               var_to_count_nonzero='Survived')\n",
    "outtab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The metrics can be put all together, for model comparison - <a href=\"https://nbviewer.jupyter.org/github/swatakit/The-public-and-private-life-of-Big-Data/blob/master/StockSentimentIndex.ipynb\"> Example</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./images/ml_workflow.png\" >\n",
    "\n",
    "# Conclusion, why Pipeline?\n",
    "\n",
    "- Short, neat, clean codes\n",
    "- Data Cleasing/Feature Engineering heaven! \n",
    "- Reusable, GridSearch-able, Serializable\n",
    "- Easy to create your own customer transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./images/thankyou.png\" >"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
